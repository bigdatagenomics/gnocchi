#!/usr/bin/env bash

set -e -x -v

# make a tempdir for writing maven cruft to
GNOCCHI_MVN_TMP_DIR=$(mktemp -d -t gnocchiTestMvnXXXXXXX)

# add this tempdir to the poms...
find . -name pom.xml \
    -exec sed -i.bak \
    -e "s:sun.io.serialization.extendedDebugInfo=true:sun.io.serialization.extendedDebugInfo=true -Djava.io.tmpdir=${GNOCCHI_MVN_TMP_DIR}:g" \
    {} \;
find . -name "*.bak" -exec rm -f {} \;

# variable declarations
export PATH=${JAVA_HOME}/bin/:${PATH}
export MAVEN_OPTS="-Xmx1536m -XX:MaxPermSize=1g -Dfile.encoding=utf-8"
DIR=$( cd $( dirname ${BASH_SOURCE[0]} ) && pwd )
PROJECT_ROOT=${DIR}/..
VERSION=$(grep "<version>" ${PROJECT_ROOT}/pom.xml  | head -2 | tail -1 | sed 's/ *<version>//g' | sed 's/<\/version>//g')

# is the hadoop version set?
if ! [[ ${HADOOP_VERSION} ]];
then
    echo "HADOOP_VERSION environment variable is not set."
    echo "Please set this variable before running."
    
    exit 1
fi

# is the spark version set?
if ! [[ ${SPARK_VERSION} ]];
then
    echo "SPARK_VERSION environment variable is not set."
    echo "Please set this variable before running."
    
    exit 1
fi

# print versions
echo "Testing GNOCCHI version ${VERSION} on Spark ${SPARK_VERSION} and Hadoop ${HADOOP_VERSION}"

# first, build the sources, run the unit tests, and generate a coverage report
mvn clean \
    -Dhadoop.version=${HADOOP_VERSION} \
    -Dspark.version=${SPARK_VERSION} 
    
# if this is a pull request, we need to set the coveralls pr id
if [[ ! -z $ghprbPullId ]];
then
    COVERALLS_PRB_OPTION="-DpullRequest=${ghprbPullId}"
fi

# coveralls token should not be visible
set +x +v

if [[ -z ${COVERALLS_REPO_TOKEN} ]];
then
    echo "Coveralls token is not set. Exiting..."
    exit 1
fi

# if those pass, build the distribution package and the integration tests
mvn -U \
    test \
    -P coverage,coveralls  scoverage:report coveralls:report \
    -DrepoToken=${COVERALLS_REPO_TOKEN} ${COVERALLS_PRB_OPTION}
      
# make verbose again
set -x -v

# we are done with maven, so clean up the maven temp dir
find ${GNOCCHI_MVN_TMP_DIR}
rm -rf ${GNOCCHI_MVN_TMP_DIR}

find . -name pom.xml \
    -exec sed -i.bak \
    -e "s:sun.io.serialization.extendedDebugInfo=true -Djava.io.tmpdir=${GNOCCHI_MVN_TMP_DIR}:sun.io.serialization.extendedDebugInfo=true:g" \
    {} \;
find . -name "*.bak" -exec rm -f {} \;

echo "$(git status --porcelain)"

if test -n "$(git status --porcelain)"
then
    echo "Applying move_to_xyz script marred a pom.xml file."
    echo "Exiting..."
    exit 1
fi

# make a temp directory
GNOCCHI_TMP_DIR=$(mktemp -d -t gnocchiTestXXXXXXX)

# Just to be paranoid.. use a directory internal to the ADAM_TMP_DIR
GNOCCHI_TMP_DIR=$GNOCCHI_TMP_DIR/deleteMePleaseThisIsNoLongerNeeded
mkdir $GNOCCHI_TMP_DIR

# set the TMPDIR envar, which is used by python to choose where to make temp directories
export TMPDIR=${GNOCCHI_TMP_DIR}

pushd $PROJECT_ROOT

# Copy the jar into our temp space for testing
cp -r . $GNOCCHI_TMP_DIR
popd

pushd $GNOCCHI_TMP_DIR

# what hadoop version are we on? format string for downloading spark assembly
if [[ $HADOOP_VERSION =~ ^2\.6 ]]; then
    HADOOP=hadoop2.6
elif [[ $HADOOP_VERSION =~ ^2\.7 ]]; then
    HADOOP=hadoop2.7
else
    echo "Unknown Hadoop version."
    exit 1
fi

# set spark artifact string for downloading assembly
SPARK=spark-${SPARK_VERSION}

# download prepackaged spark assembly
curl \
    -L "https://www.apache.org/dyn/mirrors/mirrors.cgi?action=download&filename=spark/${SPARK}/${SPARK}-bin-${HADOOP}.tgz" \
    -o ${SPARK}-bin-${HADOOP}.tgz

tar xzvf ${SPARK}-bin-${HADOOP}.tgz
export SPARK_HOME=${GNOCCHI_TMP_DIR}/${SPARK}-bin-${HADOOP}

# add pyspark to the python path
PY4J_ZIP="$(ls -1 "${SPARK_HOME}/python/lib" | grep py4j)"
export PYTHONPATH=${SPARK_HOME}/python:${SPARK_HOME}/python/lib/${PY4J_ZIP}:${PYTHONPATH}

# put gnocchi jar on the pyspark path
ASSEMBLY_DIR="${GNOCCHI_TMP_DIR}/gnocchi-assembly/target"
ASSEMBLY_JAR="$(ls -1 "$ASSEMBLY_DIR" | grep "^gnocchi[0-9A-Za-z\_\.-]*\.jar$" | grep -v javadoc | grep -v sources || true)"

export PYSPARK_SUBMIT_ARGS="--jars ${ASSEMBLY_DIR}/${ASSEMBLY_JAR} --driver-class-path ${ASSEMBLY_DIR}/${ASSEMBLY_JAR} pyspark-shell"
echo "Printing PYSPARK_SUBMIT_ARGS: \n\n ${PYSPARK_SUBMIT_ARGS} \n\n"

# create a conda environment for python build, if necessary
uuid=$(uuidgen)
conda create -q -n gnocchi-build-${uuid} python=2.7 anaconda
source activate gnocchi-build-${uuid}

# prepare python
pushd gnocchi-python
make prepare
popd

mvn -U \
    -P python \
    package \
    -DskipTests \
    -Dhadoop.version=${HADOOP_VERSION} \
    -Dspark.version=${SPARK_VERSION}

    
# copy python targets back
cp -r gnocchi-python/target ${PROJECT_ROOT}/gnocchi-python/

# deactivate and remove the conda env
source deactivate
conda remove -n gnocchi-build-${uuid} --all

pushd ${PROJECT_ROOT}
./scripts/format-source
if test -n "$(git status --porcelain)"
then
    echo "Please run './scripts/format-source'"
    exit 1
fi

echo
echo "All the tests passed"
echo
